+++
title = "EmacsでLLM"
author = ["YAMASHITA Takao"]
date = 2024-11-10T22:33:00+09:00
lastmod = 2024-11-11T08:41:26+09:00
tags = ["Emacs", "LLM-OLLAMA", "ellama"]
categories = ["Emacs", "LLM-OLLAMA", "ellama"]
draft = false
pin = false
+++

## あたらしい、おもちゃ Ollama {#あたらしい-おもちゃ-ollama}

{{<details "LLMの目的、目標と課題">}}
大規模言語モデル（LLM）は、膨大なデータと高度な計算力を使用して訓練された人工知能の一種で、特に自然言語処理（NLP）を目的としています。具体的には、文章の生成、質問応答、翻訳、要約、チャットボットとしての対話など、多岐にわたる言語タスクを遂行できます。代表的なモデルには、OpenAIのGPTシリーズやMetaのLlamaシリーズがあり、これらのモデルは数十億から数千億のパラメータを持つ大規模なネットワーク構造を備えています。

### 目的
LLMの主な目的は、以下のような言語処理タスクにおける精度と柔軟性の向上です：

1. **自然な対話生成**: 人間と似た形での会話を提供し、カスタマーサポートや情報提供の役割を果たすことが可能です。例えば、チャットボットや音声アシスタントが自然な応答を生成するためにLLMが活用されています。

2. **情報検索と要約**: 文献やニュース記事の要約を行うことで、ユーザーが迅速に必要な情報を得られるようにします。また、長文の要約やドキュメントの整理など、膨大なテキストデータの効率的な活用が期待されています。

3. **翻訳と言語変換**: 多言語対応のために、リアルタイム翻訳や特定言語の言語変換にも使われ、異なる言語間でのコミュニケーションが円滑に行えるように支援します。

### 効果と利点
大規模言語モデルは、その膨大なパラメータにより、以下のような効果が得られます：

1. **高精度の応答生成**: 大量のデータを基にした訓練により、自然で高精度な応答が生成でき、ユーザーにとって満足度の高いインタラクションを提供します。

2. **適応性の高さ**: 一度訓練されたモデルは、新しいタスクやテーマに対しても微調整（ファインチューニング）を行うことで迅速に適応し、多様な分野や用途に応用できます。

3. **スケーラビリティ**: LLMはクラウドやローカルで動作できるため、個人ユーザーから企業まで幅広い規模で利用可能です。また、応用分野は医療や教育、金融など幅広く、特定業界の課題解決にも役立ちます。

### 目標と課題
LLMには技術的な目標と同時に、いくつかの課題も存在します。

- **倫理的な考慮**: 偏見を含むデータセットで訓練されると、モデルも偏った回答を生成するリスクがあるため、フェアで倫理的な利用が重要です。
- **計算資源の要件**: LLMは大量のデータと計算資源を必要とするため、環境負荷やコスト面での課題があります。
- **データのプライバシー保護**: 特にプライバシーに敏感な情報を含む応用分野では、データの管理とセキュリティが重要です。

将来的には、より効率的で軽量なモデルの開発や、AIの倫理的・社会的責任を果たすためのガイドライン整備が目指されています。
{{</details>}}

{{<details "Ollamaって？">}}
Ollamaは、ローカルで大規模言語モデル（LLM）を実行・管理するためのソフトウェアです。ユーザーは、自身のパソコンでAIモデルを直接動かせるため、クラウドサービスに依存せず、プライバシー保護やデータの制御が可能です。Ollamaは、LlamaやCode Llamaといった人気のあるモデルをサポートしており、文章生成やコード支援など、多様な言語タスクに適しています。

Ollamaでは、「Modelfile」と呼ばれる設定ファイルを用いることで、モデルのパラメータ（例：温度設定やサンプリング手法）を細かく調整し、目的に合わせたカスタマイズが可能です。また、PythonライブラリのLangChainと統合することもでき、チャットボットや対話型AIの構築にも利用されています。

ローカルで動作するため、データの漏洩リスクが低く、インターネット接続がなくても利用できるのが利点です。ただし、大きなモデルを扱う場合には、高性能なGPUや十分なストレージが求められるため、セットアップにはある程度の技術的知識とリソースが必要です。
{{</details>}}

昨日ひさしぶりに本屋に行って立ち読みしてたら、MacのOllamaでLLMをローカルで遊ぶみたいな記事があった。
食指そそられる思いで、早速、バイナリをダウンロードして動作確認をしたところ、それなりに遊べそうだから年末年始に勉強しよう。

[Ollamaで利用可能なモデル](https://ollama.com/library) も豊富で、せっせとアップしてくれる文化があったので、Emacsで動作するパッケージもあるでしょってことで、最近お気に入りの [Perplexity](https://www.perplexity.ai) で調べた。[Google](https://www.google.com) でもいいだけど、サックと調べてくれるから。

以下、Perplexityからのコピペが混ざっているので、話し言葉と文語体が混在です😅

OllamaをmacOSにインストールし、EmacsからEllamaを設定して利用する手順を以下に示します。


### Ollamaのインストール[^fn:1] {#ollamaのインストール}

1.  **Homebrewを使用してインストール**::
    ターミナルを開き、以下のコマンドを実行します。
    ```bash
    brew install ollama
    ```

2.  **Ollamaの動作確認**::
    インストール後、次のコマンドでバージョンを確認します。
    ```bash
    ollama --version
    ```

3.  **モデルのダウンロードと実行**::
    例えば、\`gemma2:9b\`モデルを使う場合、以下のコマンドでモデルを実行します。
    ```bash
    ollama run gemma2:9b
    ```

4.  **対話を終了するには**::
    ```basn
    Ctrl + d または /bye
    ```


### EmacsでEllamaの設定 {#emacsでellamaの設定}

1.  **Emacsに必要なパッケージをインストール**:
    `use-package` を使用してEllamaを設定します。以下の設定をEmacsの設定ファイル（通常は `~/.emacs.d/init.el` ）に追加します。
    ```emacs-lisp
    (use-package ellama
      :init
      (setq ellama-keymap-prefix "C-c e")  ;; キーバインディングの設定
      (setq ellama-language "Japanese")      ;; 言語設定
      (require 'llm-ollama)
      (setq ellama-provider (make-llm-ollama
        :host "localhost"                     ;; Ollamaが動作しているホスト名
        :chat-model "gemma2:9b"               ;; 使用するチャットモデル名
        :embedding-model "gemma2:9b")))       ;; 使用する埋め込みモデル名
    ```

2.  **EmacsからEllamaを使用する**:
    Emacs内で以下のコマンドを使用して、テキスト処理を行います。
    -   テーブル作成: \`M-x ellama-make-table\`
    -   YAMLフォーマット変換: \`M-x ellama-make-format RET yaml RET\`
    -   概要作成: \`M-x ellama-summarize\`
    -   翻訳: \`M-x ellama-translate\`

私は[ leaf ](https://github.com/conao3/leaf.el)を使っているので、抜粋したものを貼り付けておく。

```emacs-lisp
(leaf ellama
  :after llm-ollama
  :ensure t
  :init
  (setopt ellama-language "Japanese")
  (setopt ellama-sessions-directory (concat no-littering-var-directory "ellama-sessions"))
  (setopt ellama-naming-scheme 'ellama-generate-name-by-llm)
  (setopt ellama-provider
          (make-llm-ollama
           ;; this model should be pulled to use it
           ;; value should be the same as you print in terminal during pull
           :chat-model "llama3:8b-instruct-q8_0"
           :embedding-model "nomic-embed-text"
           :default-chat-non-standard-params '(("num_ctx" . 8192))))
  (setopt ellama-summarization-provider
          (make-llm-ollama
           :chat-model "qwen2.5:3b"
           :embedding-model "nomic-embed-text"
           :default-chat-non-standard-params '(("num_ctx" . 32768))))
  ;; llm providers for interactive switching.
  (setopt ellama-providers
          '(("zephyr" . (make-llm-ollama
                         :chat-model "zephyr:7b-beta-q6_K"
                         :embedding-model "zephyr:7b-beta-q6_K"))
            ("mistral" . (make-llm-ollama
                          :chat-model "mistral:7b-instruct-v0.2-q6_K"
                          :embedding-model "mistral:7b-instruct-v0.2-q6_K"))
            ("mixtral" . (make-llm-ollama
                          :chat-model "mixtral:8x7b-instruct-v0.1-q3_K_M-4k"
                          :embedding-model "mixtral:8x7b-instruct-v0.1-q3_K_M-4k"))
            ("codestral" . (make-llm-ollama
                            :chat-model "codestral:22b-v0.1-q4_K_S"
                            :embedding-model "codestral:22b-v0.1-q4_K_S"))
            ("gemma2" . (make-llm-ollama
                         :chat-model "gemma2:27b-instruct-q4_K_S"
                         :embedding-model "gemma2:27b-instruct-q4_K_S"))
            ("command-r" . (make-llm-ollama
                            :chat-model "command-r:35b"
                            :embedding-model "command-r:35b"))
            ("llama3.1" . (make-llm-ollama
                           :chat-model "llama3.1:8b"
                           :embedding-model "llama3.1:8b"))))
  ;; Naming new sessions with llm
  (setopt ellama-naming-provider
          (make-llm-ollama
           :chat-model "llama3:8b-instruct-q8_0"
           :embedding-model "nomic-embed-text"
           :default-chat-non-standard-params '(("stop" . ("\n")))))
  ;; Translation llm provider
  (setopt ellama-translation-provider
          (make-llm-ollama
           :chat-model "qwen2.5:3b"
           :embedding-model "nomic-embed-text"
           :default-chat-non-standard-params
           '(("num_ctx" . 32768)))))
```


### 設定ファイル（Modelfile）の作成 {#設定ファイル-modelfile-の作成}

Ollamaでは、カスタムモデルの設定ファイルとして <span class="underline">Modelfile</span> が必要です。以下はその例です。

1.  **Modelfileの作成**:
    モデルと同じディレクトリに `Modelfile` という名前のファイルを作成し、以下の内容を記述します。

<!--listend-->

```shell
from /ollama/gemma2-9b-chat-gguf
```


## Ellamaで遊んだ {#ellamaで遊んだ}

`M+x ellama-chat` で、お遊びしたけど、ちゃんとしていて驚いた🤣


### User: {#user}

emacs29以上で動作する設定ファイル


### Ellama: {#ellama}

Emacs29から上手く動作する設定ファイルを記述します。

```el
;;; config.el - Emacs config file

;; Emacsバージョンのチェック
(unless (>= emacs-major-version 29)
  (error "Emacs 29以上をインストールしてください"))

;; 基本設定
(setq default-frame-alist '((top . 0) (left . 0) (width . 1400) (height . 800)))
(setq mouse-wheel-movement 2) ; マウスホイールの回転量
(setq tab-width 2) ; Tabキーのタブ幅
(setq indent-tabs-mode nil) ; タブを空白で置換する
(setq show-paren-mode t) ; 括弧の表示
(setq show-trailing-whitespace t) ; 不要なスペースの表示
(setq enable-pop-up-menu t) ; Pop-up menuの有効化
(setq scroll-step-1 120) ; Scrollバーのスクロール量
(setq column-number-mode t) ; 行数と列数を表示する

;; キーバイング
(global-set-key [C-left] 'previous-line) ; Ctrl + Left Arrow
(global-set-key [C-right] 'next-line') ; Ctrl + Right Arrow
(global-set-key [C-x C-k] 'kill-region') ; Ctrl + X + K
(global-set-key [C-x C-c] 'copy-region') ; Ctrl + X + C
(global-set-key [C-x C-y] 'yank') ; Ctrl + X + Y
(global-set-key [C-x C-w] 'cut-region') ; Ctrl + X + W
(global-set-key [C-c C-f] 'find-file') ; Ctrl + C + F
(global-set-key [C-c C-s] 'save-some-buffers') ; Ctrl + C + S
(global-set-key [C-c C-q] 'write-region') ; Ctrl + C + Q
(global-set-key [C-c C-j] 'next-error') ; Ctrl + C + J
(global-set-key [C-c C-i] 'previous-error') ; Ctrl + C + I
(global-set-key [C-c C-k] 'kill-some-buffers') ; Ctrl + C + K
(global-set-key [M-.] 'undo') ; Option + .
(global-set-key [M-;] 'revert') ; Option + ;
(global-set-key [C-x o] 'other-frame') ; Ctrl + X + O
(global-set-key [C-x 5] 'minibuffer-scroll-down') ; Ctrl + X + 5 (下スクロール)
(global-set-key [C-x 6] 'minibuffer-scroll-up') ; Ctrl + X + 6 (上スクロール)

;; パッケージのインストールと有効化
(package-initialize) ; Emacs Package Manager (EPM)の初期化
(add-to-list 'load-path "/path/to/package/directory") ; パッケージの探索先の追加
(require 'company) ; Company-modeパッケージの有効化
(company-setup) ; Company-modeの設定

;; テーマの選択
(set-face-attribute 'default' nil :background "black") ; 背景の色を白にする場合
(load-theme 'gruvbox-dark' t) ; テーマの有効化

;; ライブラリの有効化
(require 'org) ; Org-modeパッケージの有効化
(setq org-mode '(text . plain) (html . html)) ; Org-modeの出力形式
(add-to-list 'auto-mode-alist '("\\.org" . org-mode)) ;  ".org" で Org-mode を有効化

;; プロジェクトの管理
(require 'projectile) ; Projectileパッケージの有効化
(projectile-global-mode 1) ; Projectile をグローバルに有効化
(setq projectile-find-by-name t) ; Projectile で名前でプロジェクト検索を有効化
```

この設定ファイルでは、Emacs29から上手く動作する設定を記述していまXす。特に、エディタの基本設定、キー
バイング、パッケージのインストールと有効化、テーマの選択、ライブラリの有効化などが含まれています。こ
れを使用すると、開発者や学習者などにとって役立つ設定が提供されます。

[^fn:1]: [Ollamaのバイナリパッケージをダウンロード](https://ollama.com/download)
