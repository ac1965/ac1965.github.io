+++
title = "EmacsでLLM"
author = ["YAMASHITA Takao"]
date = 2024-11-10T22:33:00+09:00
lastmod = 2024-11-16T02:26:44+09:00
tags = ["Emacs", "LLM-OLLAMA", "ellama"]
draft = false
pin = false
+++

## あたらしい、おもちゃ Ollama {#あたらしい-おもちゃ-ollama}

{{<details "LLMの目的、目標と課題">}}
大規模言語モデル（LLM）は、膨大なデータと高度な計算力を使用して訓練された人工知能の一種で、特に自然言語処理（NLP）を目的としています。
具体的には、文章の生成、質問応答、翻訳、要約、チャットボットとしての対話など、多岐にわたる言語タスクを遂行できます。
代表的なモデルには、OpenAIのGPTシリーズやMetaのLlamaシリーズがあり、これらのモデルは数十億から数千億のパラメータを持つ大規模なネットワーク構造を備えています。

### 目的
LLMの主な目的は、以下のような言語処理タスクにおける精度と柔軟性の向上です：

1. **自然な対話生成**: 人間と似た形での会話を提供し、カスタマーサポートや情報提供の役割を果たすことが可能です。例えば、チャットボットや音声アシスタントが自然な応答を生成するためにLLMが活用されています。

2. **情報検索と要約**: 文献やニュース記事の要約を行うことで、ユーザーが迅速に必要な情報を得られるようにします。また、長文の要約やドキュメントの整理など、膨大なテキストデータの効率的な活用が期待されています。

3. **翻訳と言語変換**: 多言語対応のために、リアルタイム翻訳や特定言語の言語変換にも使われ、異なる言語間でのコミュニケーションが円滑に行えるように支援します。

### 効果と利点
大規模言語モデルは、その膨大なパラメータにより、以下のような効果が得られます：

1. **高精度の応答生成**: 大量のデータを基にした訓練により、自然で高精度な応答が生成でき、ユーザーにとって満足度の高いインタラクションを提供します。

2. **適応性の高さ**: 一度訓練されたモデルは、新しいタスクやテーマに対しても微調整（ファインチューニング）を行うことで迅速に適応し、多様な分野や用途に応用できます。

3. **スケーラビリティ**: LLMはクラウドやローカルで動作できるため、個人ユーザーから企業まで幅広い規模で利用可能です。また、応用分野は医療や教育、金融など幅広く、特定業界の課題解決にも役立ちます。

### 目標と課題
LLMには技術的な目標と同時に、いくつかの課題も存在します。

- **倫理的な考慮**: 偏見を含むデータセットで訓練されると、モデルも偏った回答を生成するリスクがあるため、フェアで倫理的な利用が重要です。
- **計算資源の要件**: LLMは大量のデータと計算資源を必要とするため、環境負荷やコスト面での課題があります。
- **データのプライバシー保護**: 特にプライバシーに敏感な情報を含む応用分野では、データの管理とセキュリティが重要です。

将来的には、より効率的で軽量なモデルの開発や、AIの倫理的・社会的責任を果たすためのガイドライン整備が目指されています。
{{</details>}}

{{<details "Ollamaって？">}}
Ollamaは、ローカルで大規模言語モデル（LLM）を実行・管理するためのソフトウェアです。
ユーザーは、自身のパソコンでAIモデルを直接動かせるため、クラウドサービスに依存せず、プライバシー保護やデータの制御が可能です。
Ollamaは、LlamaやCode Llamaといった人気のあるモデルをサポートしており、文章生成やコード支援など、多様な言語タスクに適しています。

Ollamaでは、「Modelfile」と呼ばれる設定ファイルを用いることで、モデルのパラメータ（例：温度設定やサンプリング手法）を細かく調整し、目的に合わせたカスタマイズが可能です。
また、PythonライブラリのLangChainと統合することもでき、チャットボットや対話型AIの構築にも利用されています。

ローカルで動作するため、データの漏洩リスクが低く、インターネット接続がなくても利用できるのが利点です。
ただし、大きなモデルを扱う場合には、高性能なGPUや十分なストレージが求められるため、セットアップにはある程度の技術的知識とリソースが必要です。
{{</details>}}

{{<details "Ollamaで扱うAIモデルの構造">}}
Ollamaで扱われるAIモデルの構造は、主に大規模言語モデル（LLM）に基づいています。
この構造は多層ニューラルネットワークで構成され、特にトランスフォーマーアーキテクチャを採用している点が特徴です。
トランスフォーマーは、Googleが2017年に発表したモデルで、テキストの並列処理が可能であることから、処理効率が高く、大量のデータを扱うLLMのベースとして最適です。

### トランスフォーマーの基本構造
トランスフォーマーアーキテクチャの中核部分は、エンコーダーとデコーダーの2つの部分から構成されていますが、大規模言語モデル（例：GPTやLlamaなど）ではエンコーダーのみ、
あるいはデコーダーのみを用いる場合もあります。
各層には自己注意機構（Self-Attention）があり、テキストの単語間の関係性を理解し、文脈を保持したまま処理を行うことが可能です。

- **自己注意機構**: 自己注意機構は、入力テキスト内の各単語が他の単語との関連性を持ちながら処理されるため、文章の意味や文脈を深く理解するのに役立ちます。この機構により、トランスフォーマーモデルは長い文脈でも前後関係を保持しつつ学習できるため、長文生成や複雑な文章構造の理解が可能です。

- **マルチヘッド注意**: 各層は複数の「ヘッド」と呼ばれる注意機構で構成されており、異なる視点からデータを解析することで、より多角的な意味の理解が促進されます。これにより、文脈や単語の意味の曖昧さが解消されやすくなります。

### Ollamaでのモデル管理と最適化
Ollamaでは、モデルのサイズや特性に応じて「Modelfile」を作成し、ユーザーが自由にモデル設定を変更できます。
例えば、レスポンスの多様性を制御するために「温度」パラメータを調整したり、
モデルのパフォーマンス向上のためにハードウェアアクセラレーションを有効化することも可能です。
Ollamaは、このようなモデル構造をカスタマイズ可能な設定と組み合わせることで、幅広い用途に適応させています。

### GPTやLlamaモデルの例
Ollamaでよく使用されるLlamaやGPTモデルは、以下のような特徴を持っています：

- **パラメータの多さ**: GPT-4やLlamaなどのモデルは数十億～数千億のパラメータを持ち、膨大な知識を保持しています。これは膨大なトレーニングデータと相まって、幅広い言語タスクへの対応を可能にしています。
- **データセットと学習**: トレーニングにはWebテキストや書籍など多様なデータセットが使用され、文脈の理解や知識の推論が可能です。また、特定の用途に合わせてファインチューニングができるため、ビジネスや専門分野に特化した応用も容易です。

Ollamaのようなプラットフォームでは、これらのモデル構造をうまく活用しつつ、ローカルで効率的にAIを動作させることを目指しています。
この結果、データのプライバシー保護やオフラインでの利用が可能となり、さまざまな業界での活用が進んでいます。

**Llama**と**GPT**は、いずれも大規模言語モデル（LLM）の代表例で、自然言語処理（NLP）におけるさまざまなタスクに利用される人工知能（AI）モデルです。
これらは膨大なテキストデータから学習し、人間の言語を理解し、生成する能力を持っていますが、設計思想や用途にはいくつかの違いがあります。

### Llama（Large Language Model Meta AI）
Llama（LLaMA）は、Meta（旧Facebook）によって開発されたオープンソースの大規模言語モデルです。
Llamaは、特に効率性とプライバシーに重点を置いて設計されており、GPTに比べて軽量で、ローカル環境でも動作しやすい点が特徴です。

- **設計の特徴**: Llamaはトランスフォーマーアーキテクチャを採用し、効率的な計算とモデルサイズの削減に重点を置いています。たとえば、Llama 2は数十億から数千億パラメータのモデルが提供され、エッジデバイスやローカル環境でも実行可能です。これにより、プライバシーを重視する企業や開発者が、自身のインフラ内でAIを活用できるメリットがあります。
- **用途**: Llamaは生成AI、要約、チャットボット、質問応答といった幅広いタスクに利用されており、研究機関や企業での試験的な利用も進んでいます。

### GPT（Generative Pre-trained Transformer）
GPT（特に有名なGPT-3やGPT-4）は、OpenAIによって開発された大規模言語モデルで、LLMの分野を大きく進展させました。
HAGPTは、膨大なテキストデータを基に事前学習され、テキスト生成や自然な対話が可能です。

- **設計の特徴**: GPTモデルは非常に大規模なパラメータ（GPT-4では数千億以上）を持ち、膨大な計算資源を必要とするため、通常はクラウド環境や高性能なハードウェア上で動作します。特に自己回帰型のテキスト生成が得意で、与えられたプロンプトに基づき、次の単語を予測する形で文章を生成します。
- **用途**: GPTシリーズは、対話型アシスタント、文章生成、コンテンツ制作、コード補助など、商業・学術の両方で幅広く活用されています。生成するテキストの精度や流暢さが高く、ユーザーからの問いかけに即座に応答する用途に向いています。

### LlamaとGPTの違い
- **オープンソース性と使用環境**: Llamaはオープンソースであり、特定の条件を満たせば無料で利用可能です。一方、GPTはOpenAIの提供するAPIやクラウドベースのインフラ上で利用されることが多く、利用には費用がかかることが一般的です。
- **パフォーマンスとリソース要件**: GPTは精度が高い一方で計算リソースが多く必要です。Llamaは効率化により、一般のコンピュータでも扱いやすく設計されています。

これらのモデルはどちらも高度な自然言語処理を可能にし、さまざまな分野での活用が進んでいますが、選択は使用環境やコスト、プライバシーの重要性によって決まることが多いです。
{{</details>}}

昨日ひさしぶりに本屋に行って立ち読みしてたら、MacのOllamaでLLMをローカルで遊ぶみたいな記事があった。
食指そそられる思いで、早速、バイナリをダウンロードして動作確認をしたところ、それなりに遊べそうだから年末年始に勉強しよう。

[Ollamaで利用可能なモデル](https://ollama.com/library) も豊富で、せっせとアップしてくれる文化があったので、Emacsで動作するパッケージもあるでしょってことで、最近お気に入りの [Perplexity](https://www.perplexity.ai) で調べた。[Google](https://www.google.com) でもいいだけど、サックと調べてくれるから。

以下、Perplexityからのコピペが混ざっているので、話し言葉と文語体が混在です😅

OllamaをmacOSにインストールし、EmacsからEllamaを設定して利用する手順を以下に示します。


### Ollamaのインストール[^fn:1] {#ollamaのインストール}

1.  **Homebrewを使用してインストール**::
    ターミナルを開き、以下のコマンドを実行します。
    ```bash
    brew install ollama
    ```

2.  **Ollamaの動作確認**::
    インストール後、次のコマンドでバージョンを確認します。
    ```bash
    ollama --version
    ```

3.  **モデルのダウンロードと実行**::
    例えば、\`gemma2:9b\`モデルを使う場合、以下のコマンドでモデルを実行します。
    ```bash
    ollama run gemma2:9b
    ```

4.  **対話を終了するには**::
    ```basn
    Ctrl + d または /bye
    ```


### EmacsでEllamaの設定 {#emacsでellamaの設定}

1.  **Emacsに必要なパッケージをインストール**:
    `use-package` を使用してEllamaを設定します。以下の設定をEmacsの設定ファイル（通常は `~/.emacs.d/init.el` ）に追加します。
    ```emacs-lisp
    (use-package ellama
      :init
      (setq ellama-keymap-prefix "C-c e")  ;; キーバインディングの設定
      (setq ellama-language "Japanese")      ;; 言語設定
      (require 'llm-ollama)
      (setq ellama-provider (make-llm-ollama
        :host "localhost"                     ;; Ollamaが動作しているホスト名
        :chat-model "gemma2:9b"               ;; 使用するチャットモデル名
        :embedding-model "gemma2:9b")))       ;; 使用する埋め込みモデル名
    ```

2.  **EmacsからEllamaを使用する**:
    Emacs内で以下のコマンドを使用して、テキスト処理を行います。
    -   テーブル作成: \`M-x ellama-make-table\`
    -   YAMLフォーマット変換: \`M-x ellama-make-format RET yaml RET\`
    -   概要作成: \`M-x ellama-summarize\`
    -   翻訳: \`M-x ellama-translate\`

私は[ leaf ](https://github.com/conao3/leaf.el)を使っているので、抜粋したものを貼り付けておく。

```emacs-lisp
(leaf ellama
  :after llm-ollama
  :ensure t
  :init
  (setopt ellama-language "Japanese")
  (setopt ellama-sessions-directory (concat no-littering-var-directory "ellama-sessions"))
  (setopt ellama-naming-scheme 'ellama-generate-name-by-llm)
  (setopt ellama-provider
          (make-llm-ollama
           ;; this model should be pulled to use it
           ;; value should be the same as you print in terminal during pull
           :chat-model "llama3:8b-instruct-q8_0"
           :embedding-model "nomic-embed-text"
           :default-chat-non-standard-params '(("num_ctx" . 8192))))
  (setopt ellama-summarization-provider
          (make-llm-ollama
           :chat-model "qwen2.5:3b"
           :embedding-model "nomic-embed-text"
           :default-chat-non-standard-params '(("num_ctx" . 32768))))
  ;; llm providers for interactive switching.
  (setopt ellama-providers
          '(("zephyr" . (make-llm-ollama
                         :chat-model "zephyr:7b-beta-q6_K"
                         :embedding-model "zephyr:7b-beta-q6_K"))
            ("mistral" . (make-llm-ollama
                          :chat-model "mistral:7b-instruct-v0.2-q6_K"
                          :embedding-model "mistral:7b-instruct-v0.2-q6_K"))
            ("mixtral" . (make-llm-ollama
                          :chat-model "mixtral:8x7b-instruct-v0.1-q3_K_M-4k"
                          :embedding-model "mixtral:8x7b-instruct-v0.1-q3_K_M-4k"))
            ("codestral" . (make-llm-ollama
                            :chat-model "codestral:22b-v0.1-q4_K_S"
                            :embedding-model "codestral:22b-v0.1-q4_K_S"))
            ("gemma2" . (make-llm-ollama
                         :chat-model "gemma2:27b-instruct-q4_K_S"
                         :embedding-model "gemma2:27b-instruct-q4_K_S"))
            ("command-r" . (make-llm-ollama
                            :chat-model "command-r:35b"
                            :embedding-model "command-r:35b"))
            ("llama3.1" . (make-llm-ollama
                           :chat-model "llama3.1:8b"
                           :embedding-model "llama3.1:8b"))))
  ;; Naming new sessions with llm
  (setopt ellama-naming-provider
          (make-llm-ollama
           :chat-model "llama3:8b-instruct-q8_0"
           :embedding-model "nomic-embed-text"
           :default-chat-non-standard-params '(("stop" . ("\n")))))
  ;; Translation llm provider
  (setopt ellama-translation-provider
          (make-llm-ollama
           :chat-model "qwen2.5:3b"
           :embedding-model "nomic-embed-text"
           :default-chat-non-standard-params
           '(("num_ctx" . 32768)))))
```


### 設定ファイル（Modelfile）の作成 {#設定ファイル-modelfile-の作成}

Ollamaでは、カスタムモデルの設定ファイルとして <span class="underline">Modelfile</span> が必要です。以下はその例です。

1.  **Modelfileの作成**:
    モデルと同じディレクトリに `Modelfile` という名前のファイルを作成し、以下の内容を記述します。

<!--listend-->

```shell
from /ollama/gemma2-9b-chat-gguf
```


## Ellamaで遊んだ {#ellamaで遊んだ}

`M+x ellama-chat` で、お遊びしたけど、ちゃんとしていて驚いた🤣


### User: {#user}

emacs29以上で動作する設定ファイル


### Ellama: {#ellama}

Emacs29から上手く動作する設定ファイルを記述します。

```el
;;; config.el - Emacs config file

;; Emacsバージョンのチェック
(unless (>= emacs-major-version 29)
  (error "Emacs 29以上をインストールしてください"))

;; 基本設定
(setq default-frame-alist '((top . 0) (left . 0) (width . 1400) (height . 800)))
(setq mouse-wheel-movement 2) ; マウスホイールの回転量
(setq tab-width 2) ; Tabキーのタブ幅
(setq indent-tabs-mode nil) ; タブを空白で置換する
(setq show-paren-mode t) ; 括弧の表示
(setq show-trailing-whitespace t) ; 不要なスペースの表示
(setq enable-pop-up-menu t) ; Pop-up menuの有効化
(setq scroll-step-1 120) ; Scrollバーのスクロール量
(setq column-number-mode t) ; 行数と列数を表示する

;; キーバイング
(global-set-key [C-left] 'previous-line) ; Ctrl + Left Arrow
(global-set-key [C-right] 'next-line') ; Ctrl + Right Arrow
(global-set-key [C-x C-k] 'kill-region') ; Ctrl + X + K
(global-set-key [C-x C-c] 'copy-region') ; Ctrl + X + C
(global-set-key [C-x C-y] 'yank') ; Ctrl + X + Y
(global-set-key [C-x C-w] 'cut-region') ; Ctrl + X + W
(global-set-key [C-c C-f] 'find-file') ; Ctrl + C + F
(global-set-key [C-c C-s] 'save-some-buffers') ; Ctrl + C + S
(global-set-key [C-c C-q] 'write-region') ; Ctrl + C + Q
(global-set-key [C-c C-j] 'next-error') ; Ctrl + C + J
(global-set-key [C-c C-i] 'previous-error') ; Ctrl + C + I
(global-set-key [C-c C-k] 'kill-some-buffers') ; Ctrl + C + K
(global-set-key [M-.] 'undo') ; Option + .
(global-set-key [M-;] 'revert') ; Option + ;
(global-set-key [C-x o] 'other-frame') ; Ctrl + X + O
(global-set-key [C-x 5] 'minibuffer-scroll-down') ; Ctrl + X + 5 (下スクロール)
(global-set-key [C-x 6] 'minibuffer-scroll-up') ; Ctrl + X + 6 (上スクロール)

;; パッケージのインストールと有効化
(package-initialize) ; Emacs Package Manager (EPM)の初期化
(add-to-list 'load-path "/path/to/package/directory") ; パッケージの探索先の追加
(require 'company) ; Company-modeパッケージの有効化
(company-setup) ; Company-modeの設定

;; テーマの選択
(set-face-attribute 'default' nil :background "black") ; 背景の色を白にする場合
(load-theme 'gruvbox-dark' t) ; テーマの有効化

;; ライブラリの有効化
(require 'org) ; Org-modeパッケージの有効化
(setq org-mode '(text . plain) (html . html)) ; Org-modeの出力形式
(add-to-list 'auto-mode-alist '("\\.org" . org-mode)) ;  ".org" で Org-mode を有効化

;; プロジェクトの管理
(require 'projectile) ; Projectileパッケージの有効化
(projectile-global-mode 1) ; Projectile をグローバルに有効化
(setq projectile-find-by-name t) ; Projectile で名前でプロジェクト検索を有効化
```

この設定ファイルでは、Emacs29から上手く動作する設定を記述していまXす。特に、エディタの基本設定、キー
バイング、パッケージのインストールと有効化、テーマの選択、ライブラリの有効化などが含まれています。こ
れを使用すると、開発者や学習者などにとって役立つ設定が提供されます。

[^fn:1]: [Ollamaのバイナリパッケージをダウンロード](https://ollama.com/download)
